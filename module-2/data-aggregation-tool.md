# ğŸ› ï¸ Practical Assignment: Data Aggregation Tool

## ğŸ“˜ Overview

For this project, you will create a **data aggregation tool** that will:
- Extract information from various data formats and sources
- Store the data into a **relational database (SQLite)**
- Perform **data retrieval operations**

This will involve working with **CSV**, **XML**, **JSON**, **HTML (web scraping)**, and performing **database operations**.

---

## ğŸ¯ Objectives

### 1. Data Extraction
- [ ] Extract data from a **CSV** file
- [ ] Scrape data from a given **HTML** page
- [ ] Parse an **XML** file
- [ ] Read data from a **JSON** file

### 2. Database Operations
- [ ] Create a **SQLite** database and define a schema for the data
- [ ] Insert extracted data into the SQLite database
- [ ] Implement **parameterized queries** to search for specific data

### 3. Data Interface
- [ ] Create a **command-line interface (CLI)** or a **Python script** for input/output handling

---

## ğŸ• Assignment Breakdown

### Hour 1
- Set up a **virtual environment**
- Install required libraries
- Define database **schema**
- Initiate the **SQLite** database

### Hour 2
- Write scripts to:
  - Extract data from **CSV**, **XML**, **JSON**
  - Scrape data from an **HTML** page

### Hour 3
- Insert extracted data into the **SQLite database**
- Normalize data and ensure it fits the defined schema

### Hour 4
- Write **parameterized queries** to retrieve records
- Test the full ETL (Extract â†’ Transform â†’ Load â†’ Query) flow

---

## ğŸ“¤ Submission

You must submit the following:

- âœ… Python script(s) that perform **extraction, transformation, and loading (ETL)**
- âœ… The **SQLite** database file with inserted data
- âœ… A set of **parameterized queries**
- âœ… A `requirements.txt` file with necessary Python packages
- âœ… Upload the project to **GitHub** or **GitLab**, and email the link to:  
  ğŸ“§ **henry@mabili.co.za**

---

## ğŸ“Š Evaluation Criteria

- âœ”ï¸ Correctness of data extraction and parsing  
- âœ”ï¸ Successful insertion into the database without errors  
- âœ”ï¸ Efficiency and accuracy of parameterized queries  
- âœ”ï¸ Code quality, readability, and documentation

---

## ğŸ“š Resources

- [Python Official Documentation](https://docs.python.org/3/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [BeautifulSoup Documentation (HTML Scraping)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Python modules:  
  - `csv`  
  - `json`  
  - `xml.etree.ElementTree`

---

> âš ï¸ **Remember:** Adhere to the **data usage policies** of any websites you scrape data from.

---

ğŸ‰ Good luck â€” and remember to apply best practices in **virtual environments** and **code management**!
