# 🛠️ Practical Assignment: Data Aggregation Tool

## 📘 Overview

For this project, you will create a **data aggregation tool** that will:
- Extract information from various data formats and sources
- Store the data into a **relational database (SQLite)**
- Perform **data retrieval operations**

This will involve working with **CSV**, **XML**, **JSON**, **HTML (web scraping)**, and performing **database operations**.

---

## 🎯 Objectives

### 1. Data Extraction
- [ ] Extract data from a **CSV** file
- [ ] Scrape data from a given **HTML** page
- [ ] Parse an **XML** file
- [ ] Read data from a **JSON** file

### 2. Database Operations
- [ ] Create a **SQLite** database and define a schema for the data
- [ ] Insert extracted data into the SQLite database
- [ ] Implement **parameterized queries** to search for specific data

### 3. Data Interface
- [ ] Create a **command-line interface (CLI)** or a **Python script** for input/output handling

---

## 🕐 Assignment Breakdown

### Hour 1
- Set up a **virtual environment**
- Install required libraries
- Define database **schema**
- Initiate the **SQLite** database

### Hour 2
- Write scripts to:
  - Extract data from **CSV**, **XML**, **JSON**
  - Scrape data from an **HTML** page

### Hour 3
- Insert extracted data into the **SQLite database**
- Normalize data and ensure it fits the defined schema

### Hour 4
- Write **parameterized queries** to retrieve records
- Test the full ETL (Extract → Transform → Load → Query) flow

---

## 📤 Submission

You must submit the following:

- ✅ Python script(s) that perform **extraction, transformation, and loading (ETL)**
- ✅ The **SQLite** database file with inserted data
- ✅ A set of **parameterized queries**
- ✅ A `requirements.txt` file with necessary Python packages
- ✅ Upload the project to **GitHub** or **GitLab**, and email the link to:  
  📧 **henry@mabili.co.za**

---

## 📊 Evaluation Criteria

- ✔️ Correctness of data extraction and parsing  
- ✔️ Successful insertion into the database without errors  
- ✔️ Efficiency and accuracy of parameterized queries  
- ✔️ Code quality, readability, and documentation

---

## 📚 Resources

- [Python Official Documentation](https://docs.python.org/3/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [BeautifulSoup Documentation (HTML Scraping)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- Python modules:  
  - `csv`  
  - `json`  
  - `xml.etree.ElementTree`

---

> ⚠️ **Remember:** Adhere to the **data usage policies** of any websites you scrape data from.

---

🎉 Good luck — and remember to apply best practices in **virtual environments** and **code management**!
